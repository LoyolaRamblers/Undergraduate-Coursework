---
title: "STAT 351 Final"
author: "Charles Hwang"
date: "4/30/2020"
output: html_document
---
Charles Hwang

Professor Matthews

STAT 351-001

2 May 2020

## Problems 1-3

```{r Problems 1-3, eval=FALSE}
rm(list=ls())
"Problem 1:" # Parametric statistics assume that data are derived from a distribution with defined parameters while nonparametric statistics do not. On the same note, parametric statistics also carry the assumption of normality in the data while nonparametric statistics do not which can be helpful for datasets with small sample sizes. It may be easier to conduct a statistical test with nonparametric statistics because there are fewer and more relaxed assumptions which makes it harder for them to be violated.
"Problem 2:" # ANOVA can be done with both parametric and nonparametric tests. In parametric statistics, the traditional one-way ANOVA with null hypothesis "H0: μ1 = μ2 = ... = μk" (where μ is the mean of the group and k is the number of groups being compared) and alternative hypothesis "HA: At least one μi is different" can be used. In nonparametric statistics, the equivalent Kruskal–Wallis rank-sum test with null hypothesis "H0: m1 = m2 = ... = mk" (where m is the median of the group and k is the number of groups being compared) and alternative hypothesis "HA: At least one mi is different" are used.
"Problem 3:" # A CART model is a single tree that acts as its own model and is usually pruned to avoid overfitting the data, while a random forest uses several trees, making the model less biased and more predictive. Multiple variables can be used or "tried" at each split of a random forest while each split in a CART model generally only evaluates a single variable at a time.
```

## Problem 4

```{r Problem 4}
hist(USArrests$Murder,freq=FALSE,main="Number of Murder Arrests per 100,000 People",xlab="Arrests")  # Problem 4a
legend(13,.12,c("Δ = 2.111304", "Δ = 2.595547","Δ = 1"),lwd="1",col=c("black","blue","red"))
points(density(USArrests$Murder,bw=1.06*sd(USArrests$Murder)/length(USArrests$Murder)^.2)$x,density(USArrests$Murder,bw=1.06*sd(USArrests$Murder)/length(USArrests$Murder)^.2)$y,type="l")
points(density(USArrests$Murder,bw=1.06*IQR(USArrests$Murder)/(1.34*length(USArrests$Murder)^.2))$x,density(USArrests$Murder,bw=1.06*IQR(USArrests$Murder)/(1.34*length(USArrests$Murder)^.2))$y,type="l",col="blue") # This kernel is considerably higher (0.484243, 22.9357%) and thus smoother than the other kernel, although it does not visually appear that there is much of a difference because both kernels are relatively high.
points(density(USArrests$Murder,bw=1)$x,density(USArrests$Murder,bw=1)$y,type="l",col="red") # In my opinion, an arbitrarily-chosen kernel of 1 may be better than the previous two because it doesn't just smooth over the peak of the historgram in case we needed to see more of the underlying distribution.
plot(USArrests$Murder~USArrests$UrbanPop,main="Murder Arrests per 100k vs. % Urban Population",xlab="Percent of Population Living in Urban Area",ylab="Murder Arrests per 100,000 People")                            # Problem 4b
legend("topleft",c("S = .75","S = .5","S = .2"),lwd="1",col=c("black","red","blue"))
lines(sort(USArrests$UrbanPop),fitted(loess(USArrests$Murder~USArrests$UrbanPop))[order(USArrests$UrbanPop)]) # Using a range of spans for comparison
lines(sort(USArrests$UrbanPop),fitted(loess(USArrests$Murder~USArrests$UrbanPop,span=.5))[order(USArrests$UrbanPop)],col="red") # This one appears to be the best, although it depends on what other analysis is being performed
lines(sort(USArrests$UrbanPop),fitted(loess(USArrests$Murder~USArrests$UrbanPop,span=.2))[order(USArrests$UrbanPop)],col="blue")
```

## Problem 5

```{r Problem 5,message=FALSE}
library(dslabs)
library(randomForest)
library(reshape2)
library(rpart)
library(tree)
measles <- subset(us_contagious_diseases,disease=="Measles")
measles <- melt(measles,id=c("disease","state","year"),measure="count")
measles <- dcast(measles,formula=year~state)
names(measles)[10] <- "DC"
names(measles)[31] <- "NH"
names(measles)[32] <- "NJ"
names(measles)[33] <- "NM"
names(measles)[34] <- "NY"
names(measles)[35] <- "NC"
names(measles)[36] <- "ND"
names(measles)[41] <- "RI"
names(measles)[42] <- "SC"
names(measles)[43] <- "SD"
names(measles)[50] <- "WV"
rownames(measles) <- measles$year # Assigning "year" column as index of dataset
measles$year <- NULL
set.seed(2025)                                                    # Problem 5a
tree <- rpart(Illinois~.,data=measles)
printcp(tree)
plotcp(tree)
par(mfrow=c(1,2))
plot(tree,uniform=TRUE,main="Regression Tree")
text(tree,use.n=TRUE,all=TRUE,cex=.8)
ptree <- prune(tree,cp=tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"])
plot(ptree,uniform=TRUE,main="''Pruned'' Regression Tree") # There appears to be no difference between the pruned tree and the original tree.
text(ptree,use.n=TRUE,all=TRUE,cex=.8)
par(mfrow=c(1,1))
cat("Cross validation error:",min(tree$cptable[,"xerror"]))
T <- tree(Illinois~.,data=measles) # Growing different tree
T
plot(T) # This tree has four terminal nodes instead of three which is slightly better.
text(T) # Both CART models make their first split on the number of Indiana measles cases. This makes sense because Indiana is adjacent to Illinois. However, both models then split on the number of West Virginia cases without apparent reason. The second (better) model then splits on the number of North Carolina cases on the opposite branch. This seems to imply that the number of Indiana measles cases is a primary influence on the number of Illinois cases with West Virginia and North Carolina cases as secondary influences.
set.seed(2025)                                                    # Problem 5b
rf <- randomForest(Illinois~.,data=measles,ntree=5000,importance=TRUE)
predict(rf,measles)
varImpPlot(rf,main="Variable Importance Plot",cex=.7) # Adding some graphs to visualize the random forest
plot(rf$mse,type="l")
plot(rownames(measles),measles$Illinois,main="Annual Measles Cases in Illinois, 1928-2002",xlab="Year",ylab="Number of Cases")
points(rownames(measles),predict(rf,measles),pch=0,col="green")
points(rownames(measles),predict(T),pch=2,col="brown") # Comparing random forest to CART model and actual values
abline(v=c(1963,1968),lty=3) # The measles vaccine was released in the United States in 1963 and further improved in 1968, and there appears to be significant decreases in the number of cases around these years, so I indicated these years on the plot.
segments(1965.5,c(-3000,5200),c(1965.5,2009),5200,lty=2) # Window of enlarged plot
legend(1970,89000,c("Actual Recorded Values","Predicted (Random Forest)","Predicted (CART Model)"),col=c("black","green","brown"),pch=c(1,0,2)) # The four terminal nodes of the CART model are seen in the brown triangular points plotted horizontally along four distinct imaginary lines, each representing the value of a terminal node.
plot(rownames(measles)[40:nrow(measles)],measles$Illinois[40:nrow(measles)],ylim=c(0,5000),main="Annual Measles Cases in Illinois, 1967-2002 (enlarged)",xlab="Year",ylab="Number of Cases") # Zooming in on smaller window of plot
points(rownames(measles)[40:nrow(measles)],predict(rf,measles[40:nrow(measles),]),pch=0,col="green")
points(rownames(measles)[40:nrow(measles)],predict(T)[40:nrow(measles)],pch=2,col="brown") # Comparing CART model with actual values and random forest
abline(v=1968,lty=3)
legend(1987,5000,c("Actual Recorded Values","Predicted (Random Forest)","Predicted (CART Model)"),col=c("black","green","brown"),pch=c(1,0,2)) # There is only one terminal node from the CART model visible in this plot as opposed to the full plot.
# The random forest appears to be better than the CART model at predicting annual measles cases in Illinois for nearly every year. There were a few exceptions like 1928, 1935-1936, 1967, 1969, and 1977-1978 when the actual values were closer to the terminal nodes of the CART model than the predicted values from the random forest.
cor.test(measles$Illinois,measles$Massachusetts,method="kendall") # Problem 5c
cat("Kendall's τ =",cor(measles$Illinois,measles$Massachusetts,method="kendall"))
set.seed(2025)
BS <- rep(NA,25000)
for (i in 1:25000){
  years <- rownames(measles)[sample(1:nrow(measles),nrow(measles),replace=TRUE)]
  BS[i] <- cor(measles[years,c("Illinois","Massachusetts")]$Illinois,measles[years,c("Illinois","Massachusetts")]$Massachusetts,method="kendall")
  }
cat("    Standard Error: ",sd(BS),"\nMean Squared Error: ",mean((BS-cor(measles$Illinois,measles$Massachusetts,method="kendall"))^2),"\nEstimated Bias (?): ",mean(BS-cor(measles$Illinois,measles$Massachusetts,method="kendall")),sep="")
summary(prcomp(measles))$importance["Cumulative Proportion",1:5]  # Problem 5d
plot(summary(prcomp(measles))$importance["Proportion of Variance",],main="Variance per Principal Component",xlab="Principal Component Number",ylab="Variance",ylim=c(0,1),type="o") # We can easily visualize this interpretation in a plot.
points(summary(prcomp(measles))$importance["Cumulative Proportion",],col="blue",type="o",pch=0)
abline(h=.85,lty=5) # At least four components are needed to account for 85 percent of the variability. 
segments(-2,summary(prcomp(measles))$importance["Cumulative Proportion",2],2,lty=2)
legend("center",c("Proportion of Variance","Cumulative Variance"),col=c("black","blue"),pch=c(1,0))
cat("The first two components account for approximately",summary(prcomp(measles))$importance["Proportion of Variance",1]*100,"and",summary(prcomp(measles))$importance["Proportion of Variance",2]*100,"percent of the variability, respectively. Together, the other",length(measles)-2,"components account for the remaining",(1-sum(summary(prcomp(measles))$importance["Proportion of Variance",1:2]))*100,"percent of the variability.")
par(mfrow=c(1,2))                                                 # Problem 5e
plot(hclust(dist(as.data.frame(t(measles)),method="euclidean")),main="Euclidean Method",sub="",xlab="",cex=.7) # There are six different methods of hierarchical clustering that are supported by the hclust() function. I constructed dendrograms for all of them except binary (which treated observations of "0" as "0" and all others as "1") and Minkowski (which has a power variable which produces the same dendrogram as the Euclidean method when set to the default power of p=2).
abline(h=88000,lty=2) # Choosing arbitrary cutpoints
table(cutree(hclust(dist(as.data.frame(t(measles)),method="euclidean")),h=88000)) # The dataset is transposed to use states as observations instead of years
plot(table(cutree(hclust(dist(as.data.frame(t(measles)),method="euclidean")),h=88000)),main="Cluster Distribution",xlab="Cluster",ylab="Number of States",type="o",lwd=1)
abline(h=0,lty=3)
plot(hclust(dist(as.data.frame(t(measles)),method="maximum")),main="Maximum Method",sub="",xlab="",cex=.7) 
abline(h=38000,lty=2)
table(cutree(hclust(dist(as.data.frame(t(measles)),method="maximum")),h=38000))
plot(table(cutree(hclust(dist(as.data.frame(t(measles)),method="maximum")),h=38000)),main="Cluster Distribution",xlab="Cluster",ylab="Number of States",type="o",lwd=1)
abline(h=0,lty=3)
plot(hclust(dist(as.data.frame(t(measles)),method="manhattan")),main="Manhattan Method",sub="",xlab="",cex=.7)
abline(h=420000,lty=2)
table(cutree(hclust(dist(as.data.frame(t(measles)),method="manhattan")),h=420000))
plot(table(cutree(hclust(dist(as.data.frame(t(measles)),method="manhattan")),h=420000)),main="Cluster Distribution",xlab="Cluster",ylab="Number of States",type="o",lwd=1)
abline(h=0,lty=3)
plot(hclust(dist(as.data.frame(t(measles)),method="canberra")),main="Canberra Method",sub="",xlab="",cex=.7)
abline(h=49.6,lty=2)
table(cutree(hclust(dist(as.data.frame(t(measles)),method="canberra")),h=49.6))
plot(table(cutree(hclust(dist(as.data.frame(t(measles)),method="canberra")),h=49.6)),main="Cluster Distribution",xlab="Cluster",ylab="Number of States",type="o",lwd=1)
abline(h=0,lty=3)
par(mfrow=c(1,1)) # All methods produced nearly the same results except for the Canberra method. The Euclidean, maximum, and Manhattan methods produced clusters for California, Illinois, Massachusetts, Michigan, New Jersey, New York, North Carolina, Ohio, Pennsylvania, Texas, Wisconsin, and the remaining states, with the Euclidean method pairing Illinois and Ohio and Massachusetts and New Jersey together, maximum pairing Massachusetts and New Jersey and Michigan and Wisconsin, and Manhattan pairing Illinois and Ohio, all with cutpoints at heights of thousands of units. The clustered pairs of states makes sense because they belong to the same geographic region and/or are similar (Massachusetts and New Jersey have similar square area and population sizes), and the states in their own cluster were among the ten most populated states in the country (Forida excluded), accounting for a majority of the United States' population. However, the Canberra method produced clusters for Alaska, Hawaii and Mississippi, Nevada, and five other clusters of between five and 16 states each. This is more difficult to accurately interpret because the clusters are states with seemingly no apparent relation with one another. Overall, if the dendrograms produced from the Euclidean, maximum, and Manhattan methods are combined, there are nine to 12 distinct clusters: if the three pairings of states (Illinois and Ohio, Massachusetts and New Jersey, and Michigan and Wisconsin) are all used, there are nine clusters, and if the eleven individual states are all separated, there are twelve clusters, along with the six combinations of pairings resulting in ten or eleven clusters.
```