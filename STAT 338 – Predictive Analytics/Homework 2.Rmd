---
title: "STAT 388"
output: html_document
---
Charles Hwang

Professor Matthews

STAT 388-001

20 September 2019

## Section 3.7 Exercise 3.3

```{r Exercise 3.3}
rm(list=ls())
# a) iii; given all other variables are held constant, the x5 = 10*x1*x3 term becomes negative for females if GPA is higher than 3.5.
# b) $137,100
x1 <- 4.00
x2 <- 110
x3 <- 1
x4 <- x1*x2
x5 <- x1*x3
ŷ = 50 + 20*x1 + 0.07*x2 + 35*x3 + 0.01*x4 - 10*x5
cat("$",ŷ*1000)
# c) False; although the coefficient is very small, this does not tell us how much evidence there is of an interaction effect—only that the effect itself is very small.
```

## Section 3.7 Exercise 3.8

```{r Exercise 3.8}
rm(list=ls())
library(ISLR)                  # Exercise 3.8a
lm <- lm(mpg~horsepower,data=Auto)
summary(lm)
#   (i) Yes.
#   (ii) The relationship is fairly strong with r^2 = 0.6049.
#   (iii) The relationship is negative because the coefficient for horsepower is negative (-0.157845).
#   (iv) ~24.46705 miles-per-gallon, (97.98711, 98.01289)
hp <- 98
mpg <- 39.935861 - 0.157845*hp
cat(mpg,"miles-per-gallon")
CI <- data.frame(horsepower=hp)
predict(lm,newdata=CI,interval="confidence")
PI <- data.frame(horsepower=hp)
predict(lm,newdata=PI,interval="predict")
plot(Auto$mpg~Auto$horsepower) # Exercise 3.8b
abline(lm,col="red")
par(mfrow=c(2,2))              # Exercise 3.8c
plot(lm)
# There appears to be an upwards megaphone effect in the residual graph, potentially indicating that a linear model is inadequate for the data.
```

## Section 3.7 Exercise 3.13

```{r Exercise 3.13}
rm(list=ls())
set.seed(1)
x <- rnorm(100)                # Exercise 3.13a
eps <- rnorm(100,0,sqrt(0.25)) # Exercise 3.13b
β0 <- -1                       # Exercise 3.13c
β1 <- 0.5
Y = β0 + β1*x + eps
length(Y)
plot(x,Y)                      # Exercise 3.13d
# The data form a roughly linear trend with a bulging effect in the middle.
lm <- lm(Y~x)                  # Exercise 3.13e
summary(lm)
β0h <- lm$coefficients[1]
β1h <- lm$coefficients[2]
X <- data.frame(x=β1)
predict(lm,newdata=X,interval="predict")
cat(β0h - β0,β1h - β1)
# β0-hat is 0.01884631 less than β0 and β1-hat is 0.0005301931 less than β1.
abline(lm,col="blue")          # Exercise 3.13f
abline(β0+eps,β1,col="red")
legend("topleft",legend=c("Least Squares","Population (ϵ)"),fill=c("blue","red"))
x2 <- x^2                      # Exercise 3.13g
summary(lm(Y~x+x2))
# No. The p-value for the quadratic term (0.164) is not significant at α = .05.
```

## Section 3.7 Exercise 3.15
### Exercise 3.15a

```{r Exercise 3.15a, warning=FALSE}
rm(list=ls())
library(MASS)
zn <- lm(crim~zn,data=Boston)
summary(zn)
indus <- lm(crim~indus,data=Boston)
summary(indus)
chas <- lm(crim~chas,data=Boston)
summary(chas)
nox <- lm(crim~nox,data=Boston)
summary(nox)
rm <- lm(crim~rm,data=Boston)
summary(rm)
age <- lm(crim~age,data=Boston)
summary(age)
dis <- lm(crim~dis,data=Boston)
summary(dis)
rad <- lm(crim~rad,data=Boston)
summary(rad)
tax <- lm(crim~tax,data=Boston)
summary(tax)
ptratio <- lm(crim~ptratio,data=Boston)
summary(ptratio)
black <- lm(crim~black,data=Boston)
summary(black)
lstat <- lm(crim~lstat,data=Boston)
summary(lstat)
medv <- lm(crim~medv,data=Boston)
summary(medv)
par(mfrow=c(3,5))
plot(Boston$crim~Boston$zn)
abline(zn,col="red")
plot(Boston$crim~Boston$indus)
abline(indus,col="red")
plot(Boston$crim~Boston$chas)
abline(chas,col="red")
plot(Boston$crim~Boston$nox)
abline(nox,col="red")
plot(Boston$crim~Boston$rm)
abline(rm,col="red")
plot(Boston$crim~Boston$age)
abline(age,col="red")
plot(Boston$crim~Boston$dis)
abline(dis,col="red")
plot(Boston$crim~Boston$rad)
abline(rad,col="red")
plot(Boston$crim~Boston$tax)
abline(tax,col="red")
plot(Boston$crim~Boston$ptratio)
abline(ptratio,col="red")
plot(Boston$crim~Boston$black)
abline(black,col="red")
plot(Boston$crim~Boston$lstat)
abline(lstat,col="red")
plot(Boston$crim~Boston$medv)
abline(medv,col="red")
# All of the models had a poor r^2, with the highest being the model for rad at r^2 = 0.39. The models for zn, indus, nox, rm, age, dis, rad, tax, ptratio, black, lstat, and medv had a statistically significant association between the predictor and the response variables. The model for chas did not.
```

### Exercise 3.15b

```{r Exercise 3.15b}
lm <- lm(crim~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+black+lstat+medv,data=Boston)
summary(lm)
# There are many different variables with little effect on the overall model. There could be some multicollinearity in the model. We reject H0 for variables dis, rad, and medv. There is sufficient evidence that these variables are nonzero. We fail to reject H0 for variables zn, indus, chas, nox, rm, age, tax, ptratio, black, and lstat. There is insufficient evidence that these variables are nonzero.
```

### Exercise 3.15c

```{r Exercise 3.15c}
par(mfrow=c(1,1))
x <- c(zn$coefficients[2],indus$coefficients[2],chas$coefficients[2],nox$coefficients[2],rm$coefficients[2],age$coefficients[2],dis$coefficients[2],rad$coefficients[2],tax$coefficients[2],ptratio$coefficients[2],black$coefficients[2],lstat$coefficients[2],medv$coefficients[2],data=Boston)
y <- c(lm$coefficients[1],lm$coefficients[2],lm$coefficients[3],lm$coefficients[4],lm$coefficients[5],lm$coefficients[6],lm$coefficients[7],lm$coefficients[8],lm$coefficients[9],lm$coefficients[10],lm$coefficients[11],lm$coefficients[12],lm$coefficients[13])
plot(x[1:13],y,xlab="Simple Regression Coefficient",ylab="Multiple Regression Coefficient")
abline(0,1) # Adding reference line
```

### Exercise 3.15d

```{r Exercise 3.15d}
zn2 <- Boston$zn^2
zn3 <- Boston$zn^3
summary(lm(crim~zn+zn2+zn3,data=Boston))
indus2 <- Boston$indus^2
indus3 <- Boston$indus^3
summary(lm(crim~indus+indus2+indus3,data=Boston))
chas2 <- Boston$chas^2
chas3 <- Boston$chas^3
summary(lm(crim~chas+chas2+chas3,data=Boston)) # The chas variable does not have a quadratic or cubic term because it is a dummy variable (0^2 = 0^3 = 0, 1^2 = 1^3 = 1).
nox2 <- Boston$nox^2
nox3 <- Boston$nox^3
summary(lm(crim~nox+nox2+nox3,data=Boston))
rm2 <- Boston$rm^2
rm3 <- Boston$rm^3
summary(lm(crim~rm+rm2+rm3,data=Boston))
age2 <- Boston$age^2
age3 <- Boston$age^3
summary(lm(crim~age+age2+age3,data=Boston))
dis2 <- Boston$dis^2
dis3 <- Boston$dis^3
summary(lm(crim~dis+dis2+dis3,data=Boston))
rad2 <- Boston$rad^2
rad3 <- Boston$rad^3
summary(lm(crim~rad+rad2+rad3,data=Boston))
tax2 <- Boston$tax^2
tax3 <- Boston$tax^3
summary(lm(crim~tax+tax2+tax3,data=Boston))
ptratio2 <- Boston$ptratio^2
ptratio3 <- Boston$ptratio^3
summary(lm(crim~ptratio+ptratio2+ptratio3,data=Boston))
black2 <- Boston$black^2
black3 <- Boston$black^3
summary(lm(crim~black+black2+black3,data=Boston))
lstat2 <- Boston$lstat^2
lstat3 <- Boston$lstat^3
summary(lm(crim~lstat+lstat2+lstat3,data=Boston))
medv2 <- Boston$medv^2
medv3 <- Boston$medv^3
summary(lm(crim~medv+medv2+medv3,data=Boston))
# Yes. The p-values for the quadratic terms are significant at α = .05 for the indus (0.000000000342), nox (0.00000000000000681), age (0.04738), dis (0.00000000000494), ptratio (0.00412), and medv (<0.0000000000000002) predictors. The p-values for the cubic terms are significant at α = .05 for the indus (0.00000000000120), nox (0.000000000000000696), age (0.00668), dis (0.0000000109), ptratio (0.00630), and medv (0.00000000000105) predictors.
```

## Section 5.4 Exercise 3

```{r Exercise 5.3}
# a) The data are partitioned approximately equally into "k" groups, or "folds". One fold is treated as a validation set while the remaining k - 1 folds are "trained on". This process is done k times such that each fold is treated as a validation set once.
# b) (i) k-fold cross-validation is more precise than the validation set approach because the partitions are smaller. However, this takes longer than the validation set approach.
#    (ii) k-fold cross-validation takes less time than LOOCV, especially if the data set is large, because there are less partitions to be made. However, the validation may be less precise than LOOCV.
```

## Section 5.4 Exercise 8

```{r Exercise 5.8}
rm(list=ls())
set.seed(1)    # Exercise 5.8a
y = rnorm(100)
x = rnorm(100)
  y = x - 2*x^2 + rnorm(100)
# n = 100, p = 1
# y = x - 2*x^2 + ε
plot(x,y)      # Exercise 5.8b
# There is a clear negative quadratic trend in the data.
set.seed(9)    # Exercise 5.8c
library(boot)
cv.glm(data.frame(x,y),glm(y~x,data=data.frame(x,y)))$delta                      # (i)
cv.glm(data.frame(x,y),glm(y~x+I(x^2),data=data.frame(x,y)))$delta               # (ii)
cv.glm(data.frame(x,y),glm(y~x+I(x^2)+I(x^3),data=data.frame(x,y)))$delta        # (iii)
cv.glm(data.frame(x,y),glm(y~x+I(x^2)+I(x^3)+I(x^4),data=data.frame(x,y)))$delta # (iv)
# e) The quadratic model had the smallest LOOCV error. This is what was expected because the data clearly form a quadratic pattern.
library(purrr) # Exercise 5.8f
map(list(glm(y~x,data=data.frame(x,y)),glm(y~x+I(x^2),data=data.frame(x,y)),glm(y~x+I(x^2)+I(x^3),data=data.frame(x,y)),glm(y~x+I(x^2)+I(x^3)+I(x^4),data=data.frame(x,y))),summary)
# The linear and quadratic terms are significant for all models except the linear model where the linear term is not significant, and the cubic and quartic terms are not significant for any of the models they appear in. These results agree with the conclusions from the cross-validation results, showing that the quadratic term is highly significant.
```

## Problem 9

```{r Problem 9}
rm(list=ls())
p = 1
K = 2
π1 = π2 = 0.5
# 2x(μ1 - μ2) = μ1^2 - μ2^2
# x = (μ1^2 - μ2^2) / 2(μ1 - μ2)
# x = (μ1 - μ2)(μ1 + μ2) / 2(μ1 - μ2)
# x = (μ1 + μ2) / 2
```

