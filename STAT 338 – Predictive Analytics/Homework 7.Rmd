---
title: "Homework 7"
output: html_document
---
Charles Hwang

Professor Matthews

STAT 388-001

3 December 2019

## Problem 3

```{r Problem 3}
rm(list=ls())                                                     # Problem 3a
library(e1071)
x1 <- c(3,2,4,1,2,4,4)
x2 <- c(4,2,4,4,1,3,1)
y <- c("red","red","red","red","blue","blue","blue")
plot(x1,x2,col=y,xlim=c(0,5),ylim=c(0,5))
abline(-.5,1) # -0.5 + x1 - x2 = 0                                # Problem 3b
# Classify to Blue if -0.5+x1-x2>0 and classify to Red otherwise. # Problem 3c
abline(-1,1,lty=2)                                                # Problem 3d
abline(0,1,lty=2)
# The support vectors are observations 2, 3, 5, and 6.            # Problem 3e
# A slight movement of observation 7 would not affect the maximal margin hyperplane because it is not a support vector.                                                            # Problem 3f
abline(2,1,col="red3")                                            # Problem 3g
points(1,2,col="blue")                                            # Problem 3h
```

## Problem 5

```{r Problem 5}
rm(list=ls())
set.seed(312)
x1 <- runif(500)-0.5                                             # Problem 5a
x2 <- runif(500)-0.5
y <- 1*(x1^2-x2^2>0)
plot(x1,x2,col=2+y)                                              # Problem 5b
summary(glm(y~x1+x2,family="binomial"))                          # Problem 5c
data <- data.frame(x1=x1,x2=x2,y=as.factor(y))                   # Problem 5d
plog <- rep(0,500)
plog[predict(glm(y~x1+x2,family="binomial"),data,type="response")>0.48] <- 1
plot(data[plog==1,]$x1,data[plog==1,]$x2,col=2+1,xlab="x1",ylab="x2")
points(data[plog==0,]$x1,data[plog==0,]$x2,col=2+0)
summary(glm(y~x1+I(x1^2)+x2+I(x2^2)+I(x1*x2),family="binomial")) # Problem 5e
pnl <- rep(0,500)                                                # Problem 5f
pnl[predict(glm(y~x1+I(x1^2)+x2+I(x2^2)+I(x1*x2),family="binomial"),data,type="response")>0.45] <- 1
plot(data[pnl==1,]$x1,data[pnl==1,]$x2,col=2+1,xlab="x1",ylab="x2")
points(data[pnl==0,]$x1,data[pnl==0,]$x2,col=2+0)
psvc <- predict(svm(y~x1+x2,data),data)                          # Problem 5g
plot(data[psvc==1,]$x1,data[psvc==1,]$x2,col=2+1,xlab="x1",ylab="x2")
points(data[psvc==0,]$x1,data[psvc==0,]$x2,col=2+0)
psvm <- predict(svm(y~x1+x2,data,kernel="radial",gamma=.1),data) # Problem 5h
plot(data[psvm==1,]$x1,data[psvm==1,]$x2,col=2+1,xlab="x1",ylab="x2")
points(data[psvm==0,]$x1,data[psvm==0,]$x2,col=2+0)
# The models that included the non-linear function (5f), support vector classifier (5g), non-linear kernel (5h) predict the class of observations well.                            # Problem 5i
```

## Problem 8

```{r Problem 8}
rm(list=ls())
set.seed(312)
library(ISLR)
s <- sample(nrow(OJ), 800)                                                                          # Problem 8a
train <- OJ[s,]
test <- OJ[-s,]
summary(svm(Purchase~.,train,kernel="linear",cost=.01))                                             # Problem 8b
# There are 441 support vectors; 220 are classified to CH and 221 are classified to MM.
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="linear",cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="linear",cost=.01),test))[2:3])/nrow(test))   # Problem 8c
summary(tune(svm,Purchase~.,data=train,kernel="linear",ranges=list(cost=c(seq(.01,.1,.01),.15,.2,seq(.25,10,.25)))))                                                                                               # Problem 8d
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="linear",cost=7.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="linear",cost=7.25),test))[2:3])/nrow(test)) # Problem 8e
summary(svm(Purchase~.,train,kernel="radial",cost=.01))                                              # Problem 8f
# There are 640 support vectors; 319 are classified to CH and 321 are classified to MM.
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="radial",cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="radial",cost=.01),test))[2:3])/nrow(test))
summary(tune(svm,Purchase~.,data=train,kernel="radial",ranges=list(cost=c(seq(.01,.1,.01),.15,.2,seq(.25,10,.25)))))
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="radial",cost=6.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="radial",cost=6.25),test))[2:3])/nrow(test))
summary(svm(Purchase~.,train,kernel="polynomial",degree=2,cost=.01))                                 # Problem 8g
# There are 642 support vectors; 319 are classified to CH and 323 are classified to MM.
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="polynomial",degree=2,cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="polynomial",degree=2,cost=.01),test))[2:3])/nrow(test))
summary(tune(svm,Purchase~.,data=train,kernel="polynomial",degree=2,ranges=list(cost=c(seq(.01,.1,.01),.15,.2,seq(.25,10,.25)))))
c(sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="polynomial",degree=2,cost=9.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="polynomial",degree=2,cost=9.25),test))[2:3])/nrow(test))

cat("                Train   Test \n     Linear Old",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="linear",cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="linear",cost=.01),test))[2:3])/nrow(test),"\n     Linear New",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="linear",cost=7.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="linear",cost=7.25),test))[2:3])/nrow(test),"\n     Radial Old",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="radial",cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="radial",cost=.01),test))[2:3])/nrow(test),"\n     Radial New",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="radial",cost=6.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="radial",cost=6.25),test))[2:3])/nrow(test),"\n Polynomial Old",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="polynomial",degree=2,cost=.01),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="polynomial",degree=2,cost=.01),test))[2:3])/nrow(test),"\n Polynomial New",sum(table(train$Purchase,predict(svm(Purchase~.,train,kernel="polynomial",degree=2,cost=9.25),train))[2:3])/nrow(train),sum(table(test$Purchase,predict(svm(Purchase~.,test,kernel="polynomial",degree=2,cost=9.25),test))[2:3])/nrow(test)) # The radial kernel support vector machine appears to produce the lowest training and test error rates, with a significant reduction after cost is optimized. It is also good to note that the error rates of the linear kernel machine were consistenly low regardless of cost.                                                                # Problem 8h
```