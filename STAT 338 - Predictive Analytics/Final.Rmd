---
title: "STAT 388 Final"
output: html_document
---
STAT 351/488

Predictive Analytics - Exam 2

Due December 10, 2019

Name: Charles Hwang

ID Number: 00001447912

## Problems 1-4

```{r Problems 1-4,eval=FALSE}
rm(list=ls())
"Problem 1:" # The main ideas of STAT 388: Predictive Analytics use sampling theory, non-parametric methods, multivariate analysis, and decision theory to make predictions on a given set of data. Oftentimes, a data set is randomly divided into two subsets, a training set and a test set. The training set is used to "train" and fit the model created through various methods (including—but not limited to—cross validation, bootstrapping, regression, random forests, and support vector machines), and the test set (with the response variable removed) is used to test that model.
"Problem 2:" # We may choose to use a more restrictive method like a linear model if we are afraid of overfitting (for example, if the consequences of overfitting are far greater than those of underfitting). Additionally, linear models in particular can be easier to interpret and explain.
"Problem 3:" # In supervised learning, we have a set of "n" observations with "p" predictors and a response variable "Y" and are supposed to predict the response variable for future observations. In unsupervised learning, we are not given the response variable and thus are unable to predict it. Unsupervised learning is often more challenging because of this, and we can instead choose to discover interesting trends and subgroups in the data or easier and more informative ways to visualize the data.
"Problem 4a:" # Generalized additive models (GAMs), logistic regression, principal component analysis (PCA), k-nearest neighbors (KNN), random forests, linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), CART, support vector machines (SVM), and hierarchical clustering can be used for classification problems.
"Problem 4b:" # Generalized additive models (GAMs), LASSO, ridge regression, random forests, and CART can be used for regression problems.
"Problem 4c:" # Principal component analysis (PCA) and hierarchical clustering are considered unsupervised learning techniques.
```

## Problem 5

```{r Problem 5}
data <- read.csv(file="/Users/newuser/Desktop/Notes/STAT 388/MLB2018.csv",header=TRUE,stringsAsFactors=FALSE)
data <- data[-(31:32),]
row.names(data) <- data$Tm
data <- data[,-(1:7)]
summary(prcomp(data,scale.=TRUE))                                                                                                # Problem 5a
biplot(prcomp(data,scale.=TRUE))
plot(prcomp(data,scale.=TRUE)$sdev^2/sum(prcomp(data,scale.=TRUE)$sdev^2),ylab="Explained Variance",type="o")                    # Problem 5b
# I would choose to keep 10 principal components. Ten principal components would explain over 95 percent of the variance and falls beyond the "elbow" of the scree plot while still trimming a lot of principal components that explain very little variance.
plot(cumsum(prcomp(data,scale.=TRUE)$sdev^2/sum(prcomp(data,scale.=TRUE)$sdev^2)),ylab="Cumulative Explained Variance",type="o") # Problem 5c
abline(.9,0,lty=2)
points(8,cumsum(prcomp(data,scale.=TRUE)$sdev^2/sum(prcomp(data,scale.=TRUE)$sdev^2))[8],col="green",pch=19)
cumsum(prcomp(data,scale.=TRUE)$sdev^2/sum(prcomp(data,scale.=TRUE)$sdev^2))[5:9] # Checking variance values of principal components 5-9
# Eight principal components are needed to explain at least 90 percent of the variance.
```

## Problem 6

```{r Problem 6,message=FALSE}
library(glmnet)
set.seed(1012)
datam <- model.matrix(SB~.,data=data)
ridge <- glmnet(datam,data$SB,alpha=0,lambda=10^seq(10,-2,length=100))
bestλ <- min(ridge$lambda)
error <- mean((predict(ridge,s=bestλ,newx=datam)-data$SB)^2)
c(bestλ,error)
predict(ridge,s=bestλ,newx=datam)
plot(sort(data$SB,decreasing=TRUE),xlab="",ylab="Stolen Bases",type="o")
points(sort(predict(ridge,s=bestλ,newx=datam),decreasing=TRUE),pch=0,col="red",type="o")
legend(25,135,c("Actual","Predicted"),col=c("black","red"),pch=c(1,0))
```

## Problem 7

```{r Problem 7,message=FALSE}
library(randomForest)
set.seed(1012)
rf <- randomForest(SB~.,data=data,ntree=5000,importance=TRUE) # Choosing arbitrary number of trees
rf4 <- randomForest(SB~.,data=data,ntree=5000,importance=TRUE,mtry=4) # Choosing different numbers of variables per split and plotting their MSE, out of curiosity
rf11 <- randomForest(SB~.,data=data,ntree=5000,importance=TRUE,mtry=11)
plot(rf$mse,type="l")
points(1:5000,rf4$mse,type="l",col="red")
points(1:5000,rf11$mse,type="l",col="blue")
summary(rf$mse)
varImpPlot(rf,main="Variable Importance Plot",cex=.7)
predict(rf,data)
plot(sort(data$SB,decreasing=TRUE),xlab="",ylab="Stolen Bases",type="o")
points(sort(predict(rf,data),decreasing=TRUE),pch=0,col="red",type="o")
legend(25,135,c("Actual","Predicted"),col=c("black","red"),pch=c(1,0))
```

## Problem 8

```{r Problem 8}
par(mfrow=c(1,2))
plot(sort(data$SB,decreasing=TRUE),xlab="",ylab="Stolen Bases",main="Ridge Regression Predictions",type="o")
points(sort(predict(ridge,s=bestλ,newx=datam),decreasing=TRUE),pch=0,col="red",type="o")
legend(15,135,c("Actual","Predicted"),col=c("black","red"),pch=c(1,0))
plot(sort(data$SB,decreasing=TRUE),xlab="",ylab="Stolen Bases",main="Random Forest Predictions",type="o")
points(sort(predict(rf,data),decreasing=TRUE),pch=0,col="red",type="o")
legend(15,135,c("Actual","Predicted"),col=c("black","red"),pch=c(1,0))
# It looks like the ridge regression model is better in this case.
```

## Problem 9

```{r Problem 9}
rm(list=ls())
n=30                                                                                 # Problem 9a
n
x2=0                                                                                 # Problem 9b
x2 < 0.0396957
x1=1
x1 > 0.0682064
cat("Example point: x1=",x1,", x2=",x2,sep="")
# Pruning the tree or choosing a smaller number of trees can help avoid overfitting. # Problem 9c
```

## Problems 10-11

```{r Problems 10-11,eval=FALSE}
"Problem 10:" # The number of variables per split "m", usually sqrt(p) for a classification tree and p/3 for a regression tree (where p is the total number of variables in the model), needs to be chosen. Additionally, pruning the tree would be helpful to avoid overfitting.
"Problem 11a:" # No, this classifier is not a maximal margin classifier. There is no maximal margin classifier in this case because there is no possible linear separating hyperplane.
"Problem 11b:" # The boundary could shift slightly to the right or rotate slightly counterclockwise (or both) if point "A" were to be removed, as it is clearly an outlier and likely exerting undue influence on the boundary.
```

### Problems 12-13

```{r Problems 12-13,error=TRUE}
knitr::knit_hooks$set(error = function(x, options) {
  paste0("<pre style=\"color: red;\"><code>", x, "</code></pre>")
  })
Picture_With_Dr._Matthews_And_Dr._Perry_Being_BFFs             # Problem 12
Three_Reasons_Why_Smash_Mouth_Is_The_Greatest_Band_Of_All_Time # Problem 13
```